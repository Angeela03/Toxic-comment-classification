{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8PuUrMczUt0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZQiBGVHjZge"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJp5MBVm67dN"
   },
   "source": [
    "# Setting up configuration for TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbosijqUzpAj"
   },
   "outputs": [],
   "source": [
    "use_tpu = True #@param {type:\"boolean\"}\n",
    "\n",
    "if use_tpu:\n",
    "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "else:\n",
    "  TF_MASTER=''\n",
    "\n",
    "with tf.Session(TF_MASTER) as session:\n",
    "  print ('List of devices:')\n",
    "  pprint.pprint(session.list_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxRAGBED7gic"
   },
   "source": [
    "# Mounting  google drive to access files from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qe0a_Y20xlD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2jCAEhw0-_1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/content/gdrive/My Drive/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY3QEmzF1BEm"
   },
   "outputs": [],
   "source": [
    "data_path = \"/content/gdrive/My Drive/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmHrDoq97nOR"
   },
   "source": [
    "# Getting training and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlgRPoZT1CsB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = pd.read_csv(os.path.join(data_path, \"train_file.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUzGY0qq1Eru"
   },
   "outputs": [],
   "source": [
    "test_file = pd.read_csv(os.path.join(data_path, \"test_file.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z8IdldWa3C1"
   },
   "outputs": [],
   "source": [
    "def undersampling(train_file):\n",
    "    zeroes = train_file[(train_file[['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "                                     'insult', 'identity_hate']] == 0).all(axis=1)]\n",
    "    no_zeroes = train_file[~train_file.isin(zeroes).all(1)]\n",
    "    zeroes_filtered = zeroes.sample(n=16225, random_state=1)\n",
    "    append_df = no_zeroes.append(zeroes_filtered)\n",
    "    append_df.dropna(inplace=True)\n",
    "    return append_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcXxoMl0bD5R"
   },
   "outputs": [],
   "source": [
    "# train_file = undersampling(train_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBBSd1S57s7F"
   },
   "source": [
    "# Create output labels for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moEyvS-C1Gpt"
   },
   "outputs": [],
   "source": [
    "def create_labels(data):\n",
    "    # create a list of labels\n",
    "    data = data.loc[:, 'toxic':'identity_hate']\n",
    "    list_labels = data.values.tolist()\n",
    "    list_labels = np.array(list_labels)\n",
    "    return list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV9JPlJI1JB0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiYB1LZU1M0V"
   },
   "outputs": [],
   "source": [
    "train_file.dropna(inplace=True)\n",
    "test_file.dropna(inplace=True)\n",
    "train_y = create_labels(train_file)\n",
    "test_y = create_labels(test_file)\n",
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lz8Cpp5A7_hn"
   },
   "source": [
    "# Tokenize the contents in training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iHaClBm1O9u"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "content_train = train_file[\"comment_text\"]\n",
    "content_test = test_file[\"comment_text\"]\n",
    "max_features = 20000\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(list(content_train))\n",
    "train_token = tok.texts_to_sequences(content_train)\n",
    "test_token = tok.texts_to_sequences(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zl7WD3WDWHFU"
   },
   "outputs": [],
   "source": [
    "totalNumWords = [len(i) for i in train_token]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,450,10))\n",
    "plt.xlabel(\"Word length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkGtb67AVqx2"
   },
   "source": [
    "# Make each sentences of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv3vTuWPVd4X"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 150\n",
    "train_x = pad_sequences(train_token, maxlen=max_len)\n",
    "test_x = pad_sequences(test_token, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xku1AgOf8NzX"
   },
   "source": [
    "# Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJIzRoL11RCi"
   },
   "outputs": [],
   "source": [
    "def model_lstm():\n",
    "    inp = keras.Input(shape=(max_len, ))\n",
    "    embedding = keras.layers.Embedding(max_features, 200)(inp)\n",
    "    lstm = keras.layers.Bidirectional(keras.layers.LSTM(75, name = \"LSTM_1\", return_sequences = True, dropout = 0.1, recurrent_dropout= 0.1)) (embedding)\n",
    "    max_pool = keras.layers.GlobalMaxPooling1D()(lstm)\n",
    "    dropout1 = keras.layers.Dropout(0.1)(max_pool)\n",
    "    dense  = keras.layers.Dense(50, name = \"dense1\")(dropout1)\n",
    "    dropout2 = keras.layers.Dropout(0.1)(dense)\n",
    "    leaky = keras.layers.LeakyReLU(alpha= 0.1)(dropout2)\n",
    "    output = keras.layers.Dense(6, activation=tf.nn.sigmoid, name = \"sigmoid\")(leaky)\n",
    "    model = keras.Model(inputs=inp, outputs=output)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lq1bdzy51WgO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = model_lstm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huzKO76r8Wgj"
   },
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvK1HZ0j-7i5"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "loss = \"binary_crossentropy\"\n",
    "chosen_opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "model.compile(loss=loss, optimizer=chosen_opt, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijhFzC7i8kmO"
   },
   "source": [
    "# Convert keras model to a model compatible for tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5vXg0ah_ORF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    model,\n",
    "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "         tf.contrib.cluster_resolver.TPUClusterResolver(TF_MASTER)))\n",
    "\n",
    "tpu_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXcg66G58nYt"
   },
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UPO_Y3x1YYg"
   },
   "outputs": [],
   "source": [
    "def fit_model_sampled(model, x_train, y_train,num_batch_size):\n",
    "    num_epochs = 5\n",
    "    to_shuffle = True\n",
    "    stop_early = True\n",
    "    patience = 5\n",
    "    val_split = 0.1\n",
    "    print(\"\\n-----FITTING THE MODEL-----\")\n",
    "\n",
    "    if stop_early:\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=2)\n",
    "    else:\n",
    "        early_stop = None\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best.h5\", monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "    print(\"Fitting the model:\")\n",
    "    history_object = model.fit(x_train, y_train, epochs=num_epochs,\n",
    "                               batch_size=num_batch_size,\n",
    "                               validation_split=val_split, shuffle=to_shuffle, callbacks=[early_stop, checkpoint])\n",
    "    my_model = tf.keras.models.load_model(\"best.h5\")\n",
    "\n",
    "    # K.clear_session()\n",
    "    plt.figure(figsize=(30, 15))\n",
    "    plt.plot(history_object.history['loss'], color='g', label='loss')\n",
    "    plt.plot(history_object.history['val_loss'], color='r', label='val_loss')\n",
    "    plt.plot(history_object.history['acc'], color='y', label='acc')\n",
    "    plt.plot(history_object.history['val_acc'], color='black', label='val_acc')\n",
    "    plt.plot()\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fV1QxFrp1ib6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xf9DZuAS1oP6"
   },
   "outputs": [],
   "source": [
    "fit = fit_model_sampled(tpu_model, train_x, train_y,320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUbvUZPw8tDg"
   },
   "source": [
    "# Make predictions on new test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SebtsCpo1qhH"
   },
   "outputs": [],
   "source": [
    "test_Y = test_y.copy()\n",
    "test_X = test_x.copy()\n",
    "test_X = test_X[:63896]\n",
    "test_Y = test_Y[:63896]\n",
    "predictions = fit.predict(test_X, batch_size = 2560)\n",
    "prediction_zeroes = np.zeros_like(predictions)\n",
    "prediction_zeroes[predictions >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "belJnv0y80_D"
   },
   "source": [
    "# Calculate the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHoyLRy-nYFx"
   },
   "outputs": [],
   "source": [
    "true_predictions = sum(sum(test_Y == prediction_zeroes))\n",
    "accuracy = true_predictions / (test_Y.shape[0] * test_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8wGP0FenmL3"
   },
   "outputs": [],
   "source": [
    "f1_s = [f1_score(test_Y[:, i], prediction_zeroes[:, i]) for i in range(len(prediction_zeroes[0]))]\n",
    "average_f1 = sum(f1_s) / len(f1_s)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9r-jmBfnzKR"
   },
   "outputs": [],
   "source": [
    "aucs = [roc_auc_score(test_Y[:, i], prediction_zeroes[:, i]) for i in range(len(prediction_zeroes[0]))]\n",
    "average_aucsy = sum(aucs) / len(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFfzuZ8KyM8J"
   },
   "outputs": [],
   "source": [
    "average_aucsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGNu0-c7SCaw"
   },
   "outputs": [],
   "source": [
    "prediction_zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLSRSReFSIMF"
   },
   "outputs": [],
   "source": [
    "average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WO99xBJgh6OW"
   },
   "outputs": [],
   "source": [
    "aucs = roc_auc_score(test_Y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CKtapEUGhRs"
   },
   "outputs": [],
   "source": [
    "aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xk4sIHoHGrZZ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/content/gdrive/My Drive/predictions.p\", \"wb\") as output_file:\n",
    "    pickle.dump(predictions, output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "xku1AgOf8NzX",
    "ijhFzC7i8kmO"
   ],
   "name": "LSTM_with_embedding",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
